# AToDAuDL-Seminar-Llama2

This repository contains resources for the "Advanced Topics of Data-Analysis and Deep-Learning" seminar.
The focus paper of this repository is "Llama 2: Open Foundation and Fine-Tuned Chat Models." by Touvron, Hugo, et al.

- **Llama 2 LoRA Training Notebook**: Step-by-step guide for fine-tuning Llama 2 using Low-Rank Adaptation (LoRA) in `notebooks/llama2_lora.ipynb`.
- **Sources**: Key academic references on Llama 2, LoRA, and related transformer architectures are listed for further reading.
- **Llama 2 Paper Presentation**: A PDF presentation summarizing the Llama 2 paper.

## References

[1] Touvron, Hugo, et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models." arXiv preprint arXiv:2307.09288v2 (2023). https://arxiv.org/abs/2307.09288v2  
[2] Touvron, Hugo, et al. "LLaMA: Open and Efficient Foundation Language Models." arXiv preprint arXiv:2302.13971v1 (2023). https://arxiv.org/abs/2302.13971v1  
[3] Grattafiori, Aaron, et al. "The Llama 3 Herd of Models." arXiv preprint arXiv:2407.21783v3 (2024). https://arxiv.org/abs/2407.21783v3  
[4] Vaswani, Ashish, et al. "Attention Is All You Need." arXiv preprint arXiv:1706.03762v7 (2017). https://arxiv.org/abs/1706.03762v7  
[5] Sennrich, Rico, Barry Haddow, and Alexandra Birch. "Neural Machine Translation of Rare Words with Subword Units." arXiv preprint arXiv:1508.07909v5 (2015). https://arxiv.org/abs/1508.07909v5  
[6] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners." OpenAI Blog (2019). https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf  
[7] Kudo, Taku, and John Richardson. "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing." arXiv preprint arXiv:1808.06226v1 (2018). https://arxiv.org/abs/1808.06226v1  
[8] Su, Jianlin, et al. "RoFormer: Enhanced Transformer with Rotary Position Embedding." arXiv preprint arXiv:2104.09864v5 (2023). https://arxiv.org/abs/2104.09864v5  
[9] Ainslie, Joshua, et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints." arXiv preprint arXiv:2305.13245v3 (2023). https://arxiv.org/abs/2305.13245v3  
[10] Xiong, Ruibin, et al. "On Layer Normalization in the Transformer Architecture." arXiv preprint arXiv:2002.04745v2 (2020). https://arxiv.org/abs/2002.04745v2  
[11] Zhang, Biao, and Rico Sennrich. "Root Mean Square Layer Normalization." arXiv preprint arXiv:1910.07467v1 (2019). https://arxiv.org/abs/1910.07467v1  
[12] Shazeer, Noam. "GLU Variants Improve Transformer." arXiv preprint arXiv:2002.05202v1 (2020). https://arxiv.org/abs/2002.05202v1  
[13] Loshchilov, Ilya, and Frank Hutter. "Decoupled Weight Decay Regularization." arXiv preprint arXiv:1711.05101v3 (2019). https://arxiv.org/abs/1711.05101v3  
[14] Chung, Hyung Won, et al. "Scaling Instruction-Finetuned Language Models." arXiv preprint arXiv:2210.11416v5 (2022). https://arxiv.org/abs/2210.11416v5  
[15] Schulman, John, et al. "Proximal Policy Optimization Algorithms." arXiv preprint arXiv:1707.06347v2 (2017). https://arxiv.org/abs/1707.06347v2  
[16] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." arXiv preprint arXiv:2203.02155v1 (2022). https://arxiv.org/abs/2203.02155v1  
[17] Kazemnejad, Amirhossein, et al. "The Impact of Positional Encoding on Length Generalization in Transformers." arXiv preprint arXiv:2305.19466v2 (2023). https://arxiv.org/abs/2305.19466v2  
[18] Press, Ofir, et al. "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation." arXiv preprint arXiv:2108.12409v2 (2022). https://arxiv.org/abs/2108.12409v2  
[19] Nakanishi, Ken M. "Scalable-Softmax Is Superior for Attention." arXiv preprint arXiv:2501.19399v1 (2025). https://arxiv.org/abs/2501.19399v1  
[20] Hu, Edward J., et al. "LoRA: Low-Rank Adaptation of Large Language Models." arXiv preprint arXiv:2106.09685v2 (2021). https://arxiv.org/abs/2106.09685v2  
[21] Meta AI. "Llama 4: Multimodal Intelligence." Meta AI Blog (2025). https://ai.meta.com/blog/llama-4-multimodal-intelligence/

