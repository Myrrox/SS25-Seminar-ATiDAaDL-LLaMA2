{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1044472",
   "metadata": {},
   "source": [
    "# Finetuning Llama 2 via LoRA\n",
    "\n",
    "This setup took ~17GB VRAM. With bitsandbytes 8bit quantization it can be brought down but will only work on WSL2 or Linux Native."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794dcfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab or for first-time local use (Versions are problematic this should work for nvidia series 3000+ GPUs...)\n",
    "#%pip install \"transformers==4.38.2\" \"peft==0.8.2\" torch datasets \"accelerate==0.27.2\" sentencepiece\n",
    "#%pip install torch --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996c1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, accelerate\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"accelerate\", accelerate.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA verfügbar:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"Geräte:\", torch.cuda.device_count())\n",
    "print(\"Gerätename:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Keine CUDA-GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03014f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebffaa",
   "metadata": {},
   "source": [
    "## Get model from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"NousResearch/Llama-2-7b-hf\" # This is a pre-trained model not a chat-variant (to show the difference)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12245d44",
   "metadata": {},
   "source": [
    "## Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=32, # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Common Llama2 setup\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3556",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TinyStories (use a small subset for demo)\n",
    "ds = load_dataset(\"roneneldan/TinyStories\", split=\"train[:2000]\")  # 2,000 stories\n",
    "\n",
    "# Prepare for causal LM: simple next-token prediction\n",
    "def tokenize_function(example):\n",
    "    # Use text as a single training sequence\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_ds = ds.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_ds = tokenized_ds.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal LM not masked like BERT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e1f02",
   "metadata": {},
   "source": [
    "## Set training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,                # Adjust for demonstration (increase for better fit)\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,                        # Set to True if your GPU supports bfloat16\n",
    "    fp16=False,                         # Mixed precision for speed/VRAM\n",
    "    output_dir=\"./outputs\",\n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e0900",
   "metadata": {},
   "source": [
    "## Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5567e3",
   "metadata": {},
   "source": [
    "## (Optional) Load LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove comments to load LoRA from disk\n",
    "\n",
    "#LORA_PATH = \"./lora-tinystories\"\n",
    "\n",
    "# Load tokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(LORA_PATH)\n",
    "\n",
    "# Load base model\n",
    "#base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     load_in_8bit=True,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# Load LoRA adapter on top of base model\n",
    "# model = PeftModel.from_pretrained(base_model, LORA_PATH)\n",
    "# model.eval()\n",
    "\n",
    "# print(\"Loaded LoRA adapter and tokenizer from\", LORA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fefb89",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2544210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Myr\\Documents\\VSCode\\HS\\Seminar\\AToDAuML-Seminar-Llama2\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: Write me a story about a dragon and a lama. Once upon a time, there was a tiny dragon who\n",
      "\n",
      "LoRA Finetuned Output:\n",
      " Write me a story about a dragon and a lama. Once upon a time, there was a tiny dragon who lived in a cave. The dragon was very lonely, so he decided to go on an adventure. He flew out of the cave and saw a lama. The lama was very friendly and the dragon was happy to meet him.\n",
      "\n",
      "The dragon and the lama became best friends. They played together and had lots of fun. The dragon taught the lama how to fly and the lama taught the dragon how to dance. They were the best of friends and had lots of fun together.\n",
      "\n",
      "One day, the dragon and the lama were playing when they heard a loud noise. It was a big, bad monster! The dragon and the lama were scared and\n",
      "Base Model Output:\n",
      " Write me a story about a dragon and a lama. Once upon a time, there was a tiny dragon who lived in a cave in the mountains. He was very lonely and wanted to make friends, but he was too small to fly and too shy to go out and meet anyone. One day, he decided to go to the top of the mountain and see if he could find any other dragons. As he was flying, he saw a lama sitting on a rock. The lama was meditating and the dragon was so curious that he flew down to see what the lama was doing. The lama looked up and saw the dragon. He smiled and said, “Hello, little dragon. What are you doing up here?” The dragon was so surprised that he didn’t know what to say.\n",
      "============================================================\n",
      "Prompt: Tell me a story about a small cat who learns to share her toys.\n",
      "\n",
      "LoRA Finetuned Output:\n",
      " Tell me a story about a small cat who learns to share her toys. She has a big ball and a little ball. She likes to play with them both, but sometimes she gets jealous when her friend plays with the small ball.\n",
      "\n",
      "One day, her friend comes over and wants to play with the big ball. The cat doesn't want to share, but she learns that it's not nice to be selfish. She tells her friend to play with the small ball instead.\n",
      "\n",
      "The cat's friend is happy and they play together. The cat learns that it's better to share and have fun with her friends. She smiles and purrs, and they all have a great time. The end.\n",
      "\n",
      "Bye!\n",
      "\n",
      "The moral of the\n",
      "Base Model Output:\n",
      " Tell me a story about a small cat who learns to share her toys.\n",
      "Tell me a story about a small cat who learns to share her toys.\n",
      "Tell me a story about a small cat who learns to share her toys. (2016, December 29). In WriteWork.com. Retrieved 16:11, November 20, 2017, from http://www.writework.com/essay/tell-me-story-small-cat-learns-share-her-toys\n",
      "WriteWork contributors. \"Tell me a story about a small cat who learns to share her toys\" WriteWork.com. WriteWork.com, 29 December, 20\n",
      "============================================================\n",
      "Prompt: Write a bedtime story about two best friends who go on an adventure to find a lost balloon.\n",
      "\n",
      "LoRA Finetuned Output:\n",
      " Write a bedtime story about two best friends who go on an adventure to find a lost balloon.\n",
      "\n",
      "The two friends are a little boy and a little girl. They live in a big house with lots of toys. One day, the little boy and girl find a balloon in the garden. They play with the balloon and have lots of fun.\n",
      "\n",
      "But then, the wind blows and the balloon flies away. The little boy and girl are sad and want to find the balloon. They decide to go on an adventure to find the lost balloon.\n",
      "\n",
      "The two friends walk and walk, but they can't find the balloon. They are tired and hungry. But then, they hear a noise. It's the balloon! The\n",
      "Base Model Output:\n",
      " Write a bedtime story about two best friends who go on an adventure to find a lost balloon.\n",
      "Write a bedtime story about a boy who has a dream about his future.\n",
      "Write a bedtime story about a boy who has a dream about his future. He wants to be a famous basketball player.\n",
      "Write a bedtime story about a boy who has a dream about his future. He wants to be a famous basketball player. He also wants to be a famous singer.\n",
      "Write a bedtime story about a boy who has a dream about his future. He wants to be a famous basketball player. He also wants to be a famous singer. He also wants to be a famous doctor.\n",
      "Write a bedtime story about a boy who has a dream about his future. He wants to be a famous basketball player. He also\n",
      "============================================================\n",
      "Prompt: Explain the difference between a llama and an alpaca in simple terms.\n",
      "\n",
      "LoRA Finetuned Output:\n",
      " Explain the difference between a llama and an alpaca in simple terms.\n",
      "\n",
      "An alpaca is a type of animal that is similar to a llama. Both animals have long fur and are native to South America. However, there are some differences between the two. An alpaca is smaller than a llama and has a shorter neck. Alpacas also have softer fur than llamas.\n",
      "\n",
      "Llamas are often used as pack animals, while alpacas are more commonly used for their wool. Alpacas produce a type of wool that is very soft and warm. This wool is often used to make clothing and blankets.\n",
      "\n",
      "Alpacas and llamas are both gentle animals that make good pets. They are also both\n",
      "Base Model Output:\n",
      " Explain the difference between a llama and an alpaca in simple terms.\n",
      "Llamas and alpacas are two types of camelids. Camelids are a group of animals that includes camels, llamas, and alpacas. Llamas and alpacas are both domesticated animals, but they have some differences.\n",
      "Llamas are larger than alpacas. Llamas can weigh up to 400 pounds, while alpacas can weigh up to 200 pounds. Llamas have longer legs than alpacas. Llamas also have a longer neck than alpacas.\n",
      "Alpacas are smaller than llamas. Alpacas can weigh up to 20\n",
      "============================================================\n",
      "Prompt: What is the capital of France? Give a fun fact about the city.\n",
      "\n",
      "LoRA Finetuned Output:\n",
      " What is the capital of France? Give a fun fact about the city.\n",
      "\n",
      "The capital of France is Paris. It is a very beautiful city with lots of famous landmarks. One of the most famous landmarks is the Eiffel Tower. It is a big, tall metal tower that people can go up in.\n",
      "\n",
      "Paris is also a very busy city. There are lots of people walking around and lots of cars driving on the roads. The people of Paris are very friendly and welcoming. They like to have fun and enjoy life.\n",
      "\n",
      "If you ever visit Paris, you will have a great time. There are lots of things to see and do, like visiting the Louvre Museum or taking a boat ride on the Seine River. Paris is a great city to explore and enjoy.\n",
      "Base Model Output:\n",
      " What is the capital of France? Give a fun fact about the city.\n",
      "Paris is the capital of France. The city is famous for its fashion, art, and architecture.\n",
      "What is the capital of Italy? Give a fun fact about the city.\n",
      "Rome is the capital of Italy. The city is famous for its ancient ruins, art, and architecture.\n",
      "What is the capital of Spain? Give a fun fact about the city.\n",
      "Madrid is the capital of Spain. The city is famous for its art, architecture, and nightlife.\n",
      "What is the capital of the United Kingdom? Give a fun fact about the city.\n",
      "London is the capital of the United Kingdom. The city is famous for its history, culture, and architecture.\n",
      "What is the capital of Germany\n"
     ]
    }
   ],
   "source": [
    "# - model: your LoRA-finetuned model (after training - as model is modified in-place)\n",
    "# - base_model: a fresh, original model loaded from Hugging Face (no LoRA, no finetuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.eval()\n",
    "\n",
    "# Your prompt list, easily extendable\n",
    "PROMPTS = [\n",
    "    \"Write me a story about a dragon and a lama. Once upon a time, there was a tiny dragon who\",\n",
    "    \"Tell me a story about a small cat who learns to share her toys.\",\n",
    "    \"Write a bedtime story about two best friends who go on an adventure to find a lost balloon.\",\n",
    "    \"Explain the difference between a llama and an alpaca in simple terms.\",\n",
    "    \"What is the capital of France? Give a fun fact about the city.\",\n",
    "    # Add more prompts below if you like!\n",
    "]\n",
    "\n",
    "lora_ppls = []\n",
    "base_ppls = []\n",
    "\n",
    "for prompt in PROMPTS:\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "    # LoRA finetuned model output\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    lora_gen = model.generate(input_ids, max_new_tokens=150, do_sample=True, temperature=0.7)\n",
    "    lora_output = tokenizer.decode(lora_gen[0], skip_special_tokens=True)\n",
    "    print(\"LoRA Finetuned Output:\\n\", lora_output)\n",
    "\n",
    "    # Base model output\n",
    "    base_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(base_model.device)\n",
    "    base_gen = base_model.generate(base_input_ids, max_new_tokens=150, do_sample=True, temperature=0.7)\n",
    "    base_output = tokenizer.decode(base_gen[0], skip_special_tokens=True)\n",
    "    print(\"Base Model Output:\\n\", base_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f424a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lora-tinystories\")\n",
    "tokenizer.save_pretrained(\"./lora-tinystories\")\n",
    "print(\"Adapter and tokenizer saved to ./lora-tinystories\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
